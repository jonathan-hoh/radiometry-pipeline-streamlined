# Star Tracker Validation Framework CI/CD Pipeline
# Runs validation tests on pull requests and pushes to main branch

name: Star Tracker Validation

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'validation/**'
      - 'run_validation.py'
      - '.github/workflows/validation.yml'
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'validation/**'
      - 'run_validation.py'
  workflow_dispatch:
    inputs:
      validation_scope:
        description: 'Validation scope'
        required: true
        default: 'quick'
        type: choice
        options:
        - quick
        - comprehensive
        - attitude_only
        - metrics_only

env:
  PYTHONPATH: .
  VALIDATION_LOG_LEVEL: INFO

jobs:
  # Job 1: Code Quality and Unit Tests
  code-quality:
    name: Code Quality & Unit Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('config/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        pip install pytest pytest-cov flake8 black isort
        
    - name: Code formatting check (Black)
      run: |
        black --check --diff validation/ run_validation.py
        
    - name: Import sorting check (isort)
      run: |
        isort --check-only --diff validation/ run_validation.py
        
    - name: Lint code (flake8)
      run: |
        flake8 validation/ run_validation.py --max-line-length=120 --exclude=__pycache__
        
    - name: Run unit tests
      run: |
        pytest validation/tests/ -v --cov=validation --cov-report=xml --cov-report=term
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  # Job 2: Quick Validation (for PRs)
  quick-validation:
    name: Quick Validation Suite
    runs-on: ubuntu-latest
    needs: code-quality
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && inputs.validation_scope == 'quick')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('config/requirements.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        
    - name: Create test data directory
      run: |
        mkdir -p data/PSF_sims/Gen_1
        # Create minimal test PSF files
        echo "# Test PSF data" > data/PSF_sims/Gen_1/0_deg.txt
        echo "# Test PSF data" > data/PSF_sims/Gen_1/5_deg.txt
        
    - name: Run quick validation
      run: |
        python run_validation.py --quick --module attitude --verbose
      timeout-minutes: 10
      continue-on-error: true
      
    - name: Check validation results
      run: |
        if [ -f validation/results/campaign_results_*.json ]; then
          echo "‚úÖ Validation completed successfully"
          ls -la validation/results/
        else
          echo "‚ö†Ô∏è Validation may have encountered issues"
          ls -la validation/ || true
        fi
        
    - name: Upload validation artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quick-validation-results
        path: |
          validation/results/
          validation/*.log
        retention-days: 7

  # Job 3: Comprehensive Validation (for main branch)
  comprehensive-validation:
    name: Comprehensive Validation Suite
    runs-on: ubuntu-latest
    needs: code-quality
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && inputs.validation_scope == 'comprehensive')
    
    strategy:
      matrix:
        module: [attitude, identification, astrometric, photometric, noise]
      fail-fast: false
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('config/requirements.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        
    - name: Setup test environment
      run: |
        mkdir -p data/PSF_sims/Gen_1 data/catalogs
        # Create test data files
        for angle in 0 1 2 4 5 7 8 9 11 12 14; do
          echo "# Test PSF data for ${angle} degrees" > data/PSF_sims/Gen_1/${angle}_deg.txt
        done
        echo "catalog_id,ra_deg,dec_deg,magnitude" > data/catalogs/test_catalog.csv
        echo "1,0.0,0.0,3.0" >> data/catalogs/test_catalog.csv
        
    - name: Run validation module
      run: |
        python run_validation.py --module ${{ matrix.module }} --verbose
      timeout-minutes: 30
      continue-on-error: true
      
    - name: Archive module results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: validation-results-${{ matrix.module }}
        path: |
          validation/results/${{ matrix.module }}/
          validation/*.log
        retention-days: 14

  # Job 4: Performance Benchmarking
  performance-benchmark:
    name: Performance Benchmarking
    runs-on: ubuntu-latest
    needs: code-quality
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r config/requirements.txt
        pip install memory-profiler psutil
        
    - name: Run performance benchmark
      run: |
        # Simple performance test
        time python -c "
        import time
        from validation.metrics import attitude_error_angle
        import numpy as np
        
        # Benchmark attitude error calculation
        start = time.time()
        for i in range(1000):
            q1 = np.random.randn(4)
            q1 = q1 / np.linalg.norm(q1)
            q2 = np.random.randn(4) 
            q2 = q2 / np.linalg.norm(q2)
            error = attitude_error_angle(q1, q2)
        
        duration = time.time() - start
        print(f'Performance: {1000/duration:.1f} calculations/sec')
        "
        
    - name: Memory usage test
      run: |
        python -c "
        import psutil
        import numpy as np
        from validation.monte_carlo import MonteCarloValidator
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        validator = MonteCarloValidator()
        attitudes = validator.generate_random_attitudes(1000)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f'Memory usage: {final_memory - initial_memory:.1f} MB for 1000 attitudes')
        "

  # Job 5: Results Aggregation and Reporting
  aggregate-results:
    name: Aggregate Results & Generate Report
    runs-on: ubuntu-latest
    needs: [quick-validation, comprehensive-validation]
    if: always() && (needs.quick-validation.result != 'skipped' || needs.comprehensive-validation.result != 'skipped')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all validation artifacts
      uses: actions/download-artifact@v3
      with:
        path: validation-artifacts/
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install pyyaml jinja2 matplotlib pandas
        
    - name: Generate validation report
      run: |
        python -c "
        import os
        import json
        import yaml
        from datetime import datetime
        
        # Collect validation results
        results = {}
        artifacts_dir = 'validation-artifacts'
        
        if os.path.exists(artifacts_dir):
            for item in os.listdir(artifacts_dir):
                item_path = os.path.join(artifacts_dir, item)
                if os.path.isdir(item_path):
                    print(f'Found artifact: {item}')
                    results[item] = {'status': 'completed', 'path': item_path}
        
        # Generate summary report
        report = {
            'validation_summary': {
                'timestamp': datetime.utcnow().isoformat(),
                'github_ref': os.environ.get('GITHUB_REF', 'unknown'),
                'github_sha': os.environ.get('GITHUB_SHA', 'unknown'),
                'artifacts_found': list(results.keys()),
                'total_artifacts': len(results)
            },
            'module_results': results
        }
        
        # Save report
        with open('validation_ci_report.json', 'w') as f:
            json.dump(report, f, indent=2)
            
        print(f'Generated CI validation report with {len(results)} artifacts')
        "
        
    - name: Upload validation report
      uses: actions/upload-artifact@v3
      with:
        name: validation-ci-report
        path: validation_ci_report.json
        retention-days: 30
        
    - name: Comment on PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read validation report
          let reportSummary = "## üß™ Validation Results\n\n";
          
          try {
            const reportData = JSON.parse(fs.readFileSync('validation_ci_report.json', 'utf8'));
            const summary = reportData.validation_summary;
            
            reportSummary += `- **Timestamp**: ${summary.timestamp}\n`;
            reportSummary += `- **Commit**: ${summary.github_sha.substring(0, 8)}\n`;
            reportSummary += `- **Artifacts Found**: ${summary.total_artifacts}\n`;
            reportSummary += `- **Modules Tested**: ${summary.artifacts_found.join(', ')}\n\n`;
            
            if (summary.total_artifacts > 0) {
              reportSummary += "‚úÖ Validation framework executed successfully\n\n";
              reportSummary += "üìä **Detailed Results**: Check the workflow artifacts for complete validation reports.\n";
            } else {
              reportSummary += "‚ö†Ô∏è No validation artifacts found - check workflow logs for issues.\n";
            }
            
          } catch (error) {
            reportSummary += "‚ùå Failed to parse validation report.\n";
          }
          
          reportSummary += "\n---\n*Generated by Star Tracker Validation CI*";
          
          // Post comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: reportSummary
          });

  # Job 6: Security and Dependency Check
  security-check:
    name: Security & Dependency Check
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install security tools
      run: |
        pip install safety bandit pip-audit
        
    - name: Check for known vulnerabilities
      run: |
        safety check -r config/requirements.txt
        
    - name: Run security linter
      run: |
        bandit -r validation/ run_validation.py -f json -o security-report.json
      continue-on-error: true
      
    - name: Audit pip packages
      run: |
        pip-audit --requirement config/requirements.txt --format=json --output=audit-report.json
      continue-on-error: true
      
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          security-report.json
          audit-report.json
        retention-days: 30