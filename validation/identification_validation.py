#!/usr/bin/env python3
"""
validation/identification_validation.py - Star Identification Validation

Validates star identification performance using BAST triangle matching with:
- Systematic variation of star densities and magnitude ranges
- Confusion matrix analysis (TP, FP, FN, TN)
- Identification rate vs field density characterization
- False positive analysis and spurious match detection
- Comparison to benchmark algorithms (if available)

Usage:
    from validation.identification_validation import IdentificationValidator
    
    validator = IdentificationValidator(bast_instance, catalog_interface)
    results = validator.run_identification_sweep()
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
import time
from datetime import datetime
import itertools

# Import BAST components
from src.BAST.catalog import CATALOG_DIR
from src.BAST.match import StarMatch, triangle_match
from src.BAST.identify import calculate_centroid, group_pixels
from src.multi_star.bijective_matching import optimal_star_assignment

# Import validation framework
from .metrics import identification_rate, confusion_matrix_metrics, ValidationResults
from .monte_carlo import MonteCarloValidator, ValidationScenario

logger = logging.getLogger(__name__)

@dataclass
class IdentificationConfig:
    """Configuration for star identification validation."""
    star_density_range: Tuple[int, int] = (3, 15)  # Stars in FOV
    magnitude_ranges: List[Tuple[float, float]] = None
    field_angles_deg: List[float] = None
    n_trials_per_config: int = 50
    fov_deg: float = 20.0
    catalog_limiting_magnitude: float = 6.5
    detection_threshold_sigma: float = 5.0
    output_dir: Union[str, Path] = "validation/results/identification"
    save_detailed_results: bool = True
    
    def __post_init__(self):
        if self.magnitude_ranges is None:
            self.magnitude_ranges = [(2.0, 4.0), (3.0, 5.0), (4.0, 6.0), (5.0, 6.5)]
        if self.field_angles_deg is None:
            self.field_angles_deg = [0, 5, 10, 14]
        self.output_dir = Path(self.output_dir)

@dataclass
class IdentificationTrial:
    """Results from single identification trial."""
    trial_id: int
    n_catalog_stars: int
    n_detected_stars: int
    n_matched_stars: int
    n_false_positives: int
    n_false_negatives: int
    identification_rate: float
    false_positive_rate: float
    precision: float
    recall: float
    f1_score: float
    magnitude_range: Tuple[float, float]
    field_angle_deg: float
    execution_time: float
    success: bool
    error_message: Optional[str] = None
    detailed_matches: Optional[List[Dict[str, Any]]] = None

class IdentificationValidator:
    """
    Star identification performance validation using BAST algorithms.
    
    Validates identification performance by:
    1. Generating synthetic star fields with varying density and magnitudes
    2. Running BAST triangle matching and identification
    3. Computing confusion matrices and performance metrics
    4. Analyzing false positive patterns and failure modes
    """
    
    def __init__(
        self,
        bast_instance: Any,
        catalog_interface: Any,
        config: Optional[IdentificationConfig] = None
    ):
        """
        Initialize identification validator.
        
        Parameters
        ----------
        bast_instance : Any
            BAST algorithm instance (triangle matching)
        catalog_interface : Any
            Star catalog interface for ground truth
        config : IdentificationConfig, optional
            Validation configuration
        """
        self.bast = bast_instance
        self.catalog = catalog_interface
        self.config = config or IdentificationConfig()
        
        # Create output directory
        self.config.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"IdentificationValidator initialized for {self.config.star_density_range} stars")
        
    def run_identification_sweep(
        self,
        star_densities: Optional[List[int]] = None,
        magnitude_ranges: Optional[List[Tuple[float, float]]] = None
    ) -> Dict[str, Any]:
        """
        Run systematic identification performance sweep.
        
        Parameters
        ----------
        star_densities : List[int], optional
            Number of stars in FOV to test
        magnitude_ranges : List[Tuple[float, float]], optional
            Magnitude ranges to test
            
        Returns
        -------
        Dict[str, Any]
            Comprehensive identification validation results
        """
        logger.info("Starting identification performance sweep")
        
        if star_densities is None:
            star_densities = list(range(
                self.config.star_density_range[0],
                self.config.star_density_range[1] + 1
            ))
            
        if magnitude_ranges is None:
            magnitude_ranges = self.config.magnitude_ranges
            
        all_trials = []
        sweep_start = time.time()
        
        # Generate all test configurations
        test_configs = list(itertools.product(
            star_densities,
            magnitude_ranges,
            self.config.field_angles_deg
        ))
        
        total_configs = len(test_configs)
        logger.info(f"Testing {total_configs} configurations with {self.config.n_trials_per_config} trials each")
        
        for config_idx, (n_stars, mag_range, field_angle) in enumerate(test_configs):
            logger.info(f"Config {config_idx+1}/{total_configs}: {n_stars} stars, "
                       f"mag {mag_range[0]:.1f}-{mag_range[1]:.1f}, {field_angle}Â°")
            
            config_trials = self._run_configuration_trials(
                n_stars=n_stars,
                magnitude_range=mag_range,
                field_angle_deg=field_angle,
                n_trials=self.config.n_trials_per_config
            )
            
            all_trials.extend(config_trials)
            
            # Progress logging
            if (config_idx + 1) % 5 == 0:
                elapsed = time.time() - sweep_start
                eta = elapsed / (config_idx + 1) * total_configs - elapsed
                logger.info(f"Progress: {config_idx+1}/{total_configs} configs - ETA: {eta:.1f}s")
        
        # Compute comprehensive statistics
        statistics = self._compute_identification_statistics(all_trials)
        
        # Generate analysis plots
        plot_paths = self._generate_identification_plots(all_trials)
        
        # False positive analysis
        fp_analysis = self._analyze_false_positives(all_trials)
        
        # Save detailed results
        if self.config.save_detailed_results:
            results_file = self._save_detailed_results(all_trials)
        else:
            results_file = None
            
        sweep_time = time.time() - sweep_start
        
        # Compile final results
        validation_results = {
            'sweep_summary': {
                'total_configurations': total_configs,
                'total_trials': len(all_trials),
                'successful_trials': sum(1 for t in all_trials if t.success),
                'sweep_duration_seconds': sweep_time,
                'timestamp': datetime.utcnow().isoformat()
            },
            'statistics': statistics,
            'false_positive_analysis': fp_analysis,
            'file_outputs': {
                'plot_files': plot_paths,
                'detailed_results': str(results_file) if results_file else None
            },
            'configuration': {
                'star_density_range': self.config.star_density_range,
                'magnitude_ranges': self.config.magnitude_ranges,
                'field_angles_deg': self.config.field_angles_deg,
                'n_trials_per_config': self.config.n_trials_per_config
            }
        }
        
        logger.info(f"Identification sweep completed in {sweep_time:.1f}s")
        return validation_results
    
    def compute_confusion_matrix(
        self,
        true_ids: List[int],
        solved_ids: List[int],
        detected_ids: List[int]
    ) -> Dict[str, Any]:
        """
        Compute confusion matrix for star identification.
        
        Parameters
        ----------
        true_ids : List[int]
            Ground truth catalog star IDs in FOV
        solved_ids : List[int]
            Successfully identified star IDs
        detected_ids : List[int]
            All detected star IDs (including false positives)
            
        Returns
        -------
        Dict[str, Any]
            Confusion matrix elements and derived metrics
        """
        true_set = set(true_ids)
        solved_set = set(solved_ids)
        detected_set = set(detected_ids)
        
        # Confusion matrix elements
        true_positives = len(solved_set & true_set)  # Correctly identified
        false_positives = len(detected_set - true_set)  # Spurious detections
        false_negatives = len(true_set - solved_set)  # Missed catalog stars
        
        # True negatives are harder to define for star identification
        # Use estimate based on background detection probability
        estimated_background_detections = max(0, len(detected_set) - len(true_set))
        true_negatives = 0  # Conservative estimate
        
        # Compute derived metrics
        metrics = confusion_matrix_metrics(
            true_positives, false_positives, false_negatives, true_negatives
        )
        
        confusion_matrix = {
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'true_negatives': true_negatives,
            'precision': metrics['precision'],
            'recall': metrics['recall'],
            'f1_score': metrics['f1_score'],
            'identification_rate': true_positives / len(true_set) if true_set else 0.0,
            'false_positive_rate': false_positives / len(detected_set) if detected_set else 0.0
        }
        
        return confusion_matrix
    
    def identification_rate_vs_density(
        self,
        density_range: Tuple[int, int] = (3, 15),
        magnitude_range: Tuple[float, float] = (3.0, 5.0),
        n_trials: int = 100
    ) -> Dict[str, Any]:
        """
        Characterize identification rate vs star field density.
        
        Parameters
        ----------
        density_range : Tuple[int, int]
            Range of star densities to test
        magnitude_range : Tuple[float, float]
            Magnitude range for test stars
        n_trials : int
            Number of trials per density
            
        Returns
        -------
        Dict[str, Any]
            Identification rate vs density analysis
        """
        logger.info(f"Analyzing identification rate vs density: {density_range}")
        
        densities = list(range(density_range[0], density_range[1] + 1))
        density_results = {}
        
        for n_stars in densities:
            trials = self._run_configuration_trials(
                n_stars=n_stars,
                magnitude_range=magnitude_range,
                field_angle_deg=0.0,  # On-axis performance
                n_trials=n_trials
            )
            
            successful_trials = [t for t in trials if t.success]
            if successful_trials:
                id_rates = [t.identification_rate for t in successful_trials]
                fp_rates = [t.false_positive_rate for t in successful_trials]
                
                density_results[n_stars] = {
                    'mean_identification_rate': np.mean(id_rates),
                    'std_identification_rate': np.std(id_rates),
                    'mean_false_positive_rate': np.mean(fp_rates),
                    'std_false_positive_rate': np.std(fp_rates),
                    'success_rate': len(successful_trials) / len(trials),
                    'n_trials': len(trials)
                }
            else:
                density_results[n_stars] = {
                    'mean_identification_rate': 0.0,
                    'std_identification_rate': 0.0,
                    'mean_false_positive_rate': 1.0,
                    'std_false_positive_rate': 0.0,
                    'success_rate': 0.0,
                    'n_trials': len(trials)
                }
        
        # Generate density plot
        plot_path = self._plot_density_analysis(density_results)
        
        return {
            'density_analysis': density_results,
            'plot_file': str(plot_path),
            'test_parameters': {
                'density_range': density_range,
                'magnitude_range': magnitude_range,
                'n_trials_per_density': n_trials
            }
        }
    
    def false_positive_analysis(
        self,
        trial_results: List[IdentificationTrial]
    ) -> Dict[str, Any]:
        """
        Analyze false positive patterns and failure modes.
        
        Parameters
        ----------
        trial_results : List[IdentificationTrial]
            Trial results to analyze
            
        Returns
        -------
        Dict[str, Any]
            False positive analysis results
        """
        logger.info("Analyzing false positive patterns")
        
        # Extract false positive data
        fp_data = []
        for trial in trial_results:
            if trial.success and hasattr(trial, 'detailed_matches') and trial.detailed_matches:
                for match in trial.detailed_matches:
                    if match.get('is_false_positive', False):
                        fp_data.append({
                            'trial_id': trial.trial_id,
                            'magnitude_range': trial.magnitude_range,
                            'field_angle': trial.field_angle_deg,
                            'match_confidence': match.get('confidence', 0.0),
                            'geometric_consistency': match.get('geometric_consistency', 0.0),
                            'brightness_ratio': match.get('brightness_ratio', 1.0)
                        })
        
        if not fp_data:
            return {
                'total_false_positives': 0,
                'analysis': 'No false positives with detailed data available'
            }
        
        fp_df = pd.DataFrame(fp_data)
        
        # False positive statistics
        fp_analysis = {
            'total_false_positives': len(fp_data),
            'false_positives_per_trial': len(fp_data) / len([t for t in trial_results if t.success]),
            'confidence_distribution': {
                'mean': float(fp_df['match_confidence'].mean()),
                'std': float(fp_df['match_confidence'].std()),
                'median': float(fp_df['match_confidence'].median()),
                'min': float(fp_df['match_confidence'].min()),
                'max': float(fp_df['match_confidence'].max())
            },
            'field_angle_breakdown': fp_df.groupby('field_angle')['match_confidence'].agg(['count', 'mean']).to_dict(),
            'magnitude_range_breakdown': {}
        }
        
        # Analyze by magnitude range
        for mag_range in set(tuple(t.magnitude_range) for t in trial_results):
            mag_fps = fp_df[fp_df['magnitude_range'].apply(tuple) == mag_range]
            if len(mag_fps) > 0:
                fp_analysis['magnitude_range_breakdown'][f"{mag_range[0]:.1f}-{mag_range[1]:.1f}"] = {
                    'count': len(mag_fps),
                    'mean_confidence': float(mag_fps['match_confidence'].mean())
                }
        
        return fp_analysis
    
    def compare_to_benchmark_algorithms(
        self,
        benchmark_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Compare BAST performance to literature benchmark algorithms.
        
        Parameters
        ----------
        benchmark_data : Dict[str, Any], optional
            Literature benchmark data for comparison
            
        Returns
        -------
        Dict[str, Any]
            Benchmark comparison results
        """
        logger.info("Comparing to benchmark algorithms")
        
        if benchmark_data is None:
            # Default literature benchmarks (example values)
            benchmark_data = {
                'Liebe_1995': {
                    'identification_rate': 0.95,
                    'false_positive_rate': 0.02,
                    'processing_time_ms': 50,
                    'conditions': 'Bright stars (mag < 4.0), nominal conditions'
                },
                'Pyramid_2004': {
                    'identification_rate': 0.93,
                    'false_positive_rate': 0.05,
                    'processing_time_ms': 30,
                    'conditions': 'Mixed magnitudes, geometric algorithm'
                },
                'Grid_Algorithm': {
                    'identification_rate': 0.90,
                    'false_positive_rate': 0.03,
                    'processing_time_ms': 100,
                    'conditions': 'Grid-based matching'
                }
            }
        
        # This would require actual trial data to make meaningful comparisons
        # For now, provide structure for comparison analysis
        
        comparison_results = {
            'benchmark_algorithms': benchmark_data,
            'bast_performance': {
                'note': 'BAST performance would be computed from actual trial results',
                'placeholder_metrics': {
                    'identification_rate': 'TBD from trial data',
                    'false_positive_rate': 'TBD from trial data',
                    'processing_time_ms': 'TBD from trial data'
                }
            },
            'relative_performance': {
                'note': 'Relative performance analysis requires trial data input'
            }
        }
        
        return comparison_results
    
    def _run_configuration_trials(
        self,
        n_stars: int,
        magnitude_range: Tuple[float, float],
        field_angle_deg: float,
        n_trials: int
    ) -> List[IdentificationTrial]:
        """Run trials for specific configuration."""
        trials = []
        
        for trial_id in range(n_trials):
            try:
                # Generate synthetic star field
                star_field = self._generate_star_field(
                    n_stars=n_stars,
                    magnitude_range=magnitude_range,
                    field_angle_deg=field_angle_deg,
                    trial_seed=trial_id
                )
                
                # Run detection simulation
                detection_results = self._simulate_star_detection(
                    star_field, trial_seed=trial_id
                )
                
                # Run BAST identification
                identification_results = self._run_bast_identification(
                    detection_results['detected_stars'],
                    star_field['catalog_stars']
                )
                
                # Compute confusion matrix
                confusion = self.compute_confusion_matrix(
                    true_ids=[s['catalog_id'] for s in star_field['catalog_stars']],
                    solved_ids=[m['catalog_id'] for m in identification_results['matches']],
                    detected_ids=[s['detection_id'] for s in detection_results['detected_stars']]
                )
                
                # Create trial result
                trial = IdentificationTrial(
                    trial_id=trial_id,
                    n_catalog_stars=len(star_field['catalog_stars']),
                    n_detected_stars=len(detection_results['detected_stars']),
                    n_matched_stars=confusion['true_positives'],
                    n_false_positives=confusion['false_positives'],
                    n_false_negatives=confusion['false_negatives'],
                    identification_rate=confusion['identification_rate'],
                    false_positive_rate=confusion['false_positive_rate'],
                    precision=confusion['precision'],
                    recall=confusion['recall'],
                    f1_score=confusion['f1_score'],
                    magnitude_range=magnitude_range,
                    field_angle_deg=field_angle_deg,
                    execution_time=identification_results['execution_time'],
                    success=True,
                    detailed_matches=identification_results.get('detailed_matches')
                )
                
                trials.append(trial)
                
            except Exception as e:
                logger.error(f"Trial {trial_id} failed: {e}")
                trial = IdentificationTrial(
                    trial_id=trial_id,
                    n_catalog_stars=0,
                    n_detected_stars=0,
                    n_matched_stars=0,
                    n_false_positives=0,
                    n_false_negatives=0,
                    identification_rate=0.0,
                    false_positive_rate=0.0,
                    precision=0.0,
                    recall=0.0,
                    f1_score=0.0,
                    magnitude_range=magnitude_range,
                    field_angle_deg=field_angle_deg,
                    execution_time=0.0,
                    success=False,
                    error_message=str(e)
                )
                trials.append(trial)
        
        return trials
    
    def _generate_star_field(
        self,
        n_stars: int,
        magnitude_range: Tuple[float, float],
        field_angle_deg: float,
        trial_seed: int
    ) -> Dict[str, Any]:
        """Generate synthetic star field for testing."""
        np.random.seed(trial_seed)
        
        # Generate catalog stars
        catalog_stars = []
        for i in range(n_stars):
            star = {
                'catalog_id': i,
                'ra_deg': np.random.uniform(0, 360),
                'dec_deg': np.random.uniform(-90, 90),
                'magnitude': np.random.uniform(magnitude_range[0], magnitude_range[1]),
                'spectral_type': 'G5',  # Simplified
                'position_uncertainty_arcsec': 0.1
            }
            catalog_stars.append(star)
        
        return {
            'catalog_stars': catalog_stars,
            'field_angle_deg': field_angle_deg,
            'fov_deg': self.config.fov_deg,
            'generation_seed': trial_seed
        }
    
    def _simulate_star_detection(
        self,
        star_field: Dict[str, Any],
        trial_seed: int
    ) -> Dict[str, Any]:
        """Simulate star detection with realistic noise and detection probability."""
        np.random.seed(trial_seed + 1000)  # Different seed from generation
        
        detected_stars = []
        detection_id = 0
        
        for catalog_star in star_field['catalog_stars']:
            # Detection probability based on magnitude and field angle
            mag = catalog_star['magnitude']
            field_angle = star_field['field_angle_deg']
            
            # Simple detection probability model
            detection_prob = np.exp(-(mag - 2.0) / 2.0) * np.exp(-field_angle / 20.0)
            detection_prob = min(0.98, max(0.1, detection_prob))
            
            if np.random.random() < detection_prob:
                # Add detection noise
                detected_star = {
                    'detection_id': detection_id,
                    'catalog_id': catalog_star['catalog_id'],  # For truth tracking
                    'centroid_x': np.random.uniform(50, 950),  # Pixel coordinates
                    'centroid_y': np.random.uniform(50, 950),
                    'magnitude_measured': mag + np.random.normal(0, 0.1),
                    'snr': 10**(2.0 - 0.4 * mag),  # Simplified SNR
                    'detection_confidence': detection_prob
                }
                detected_stars.append(detected_star)
                detection_id += 1
        
        # Add false detections (noise sources)
        n_false_detections = np.random.poisson(1.0)  # Average 1 false detection
        for i in range(n_false_detections):
            false_star = {
                'detection_id': detection_id,
                'catalog_id': -1,  # Indicates false detection
                'centroid_x': np.random.uniform(50, 950),
                'centroid_y': np.random.uniform(50, 950),
                'magnitude_measured': np.random.uniform(5.0, 7.0),
                'snr': np.random.uniform(3, 8),
                'detection_confidence': np.random.uniform(0.1, 0.6)
            }
            detected_stars.append(false_star)
            detection_id += 1
        
        return {
            'detected_stars': detected_stars,
            'detection_algorithm': 'simulated',
            'detection_threshold_sigma': self.config.detection_threshold_sigma
        }
    
    def _run_bast_identification(
        self,
        detected_stars: List[Dict[str, Any]],
        catalog_stars: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Run BAST identification algorithm."""
        start_time = time.time()
        
        # Placeholder BAST identification
        # In real implementation, this would use actual BAST algorithms
        
        matches = []
        detailed_matches = []
        
        # Simple matching simulation based on position proximity
        for detected in detected_stars:
            best_match = None
            best_distance = float('inf')
            
            for catalog in catalog_stars:
                # Simplified distance metric (would use proper angular distance)
                distance = np.sqrt(
                    (detected['centroid_x'] - catalog['catalog_id'] * 100)**2 +
                    (detected['centroid_y'] - catalog['catalog_id'] * 80)**2
                )
                
                if distance < best_distance and distance < 50:  # Matching threshold
                    best_distance = distance
                    best_match = catalog
            
            if best_match is not None:
                # Check if this is a true match or false positive
                is_true_match = detected['catalog_id'] == best_match['catalog_id']
                
                match = {
                    'detection_id': detected['detection_id'],
                    'catalog_id': best_match['catalog_id'],
                    'confidence': np.exp(-best_distance / 20.0),
                    'geometric_consistency': np.random.uniform(0.7, 1.0),
                    'brightness_ratio': detected['magnitude_measured'] / best_match['magnitude']
                }
                matches.append(match)
                
                detailed_match = match.copy()
                detailed_match['is_false_positive'] = not is_true_match
                detailed_matches.append(detailed_match)
        
        execution_time = time.time() - start_time
        
        return {
            'matches': matches,
            'detailed_matches': detailed_matches,
            'execution_time': execution_time,
            'algorithm': 'BAST_triangle_matching',
            'matching_threshold': 50  # pixels
        }
    
    def _compute_identification_statistics(
        self,
        all_trials: List[IdentificationTrial]
    ) -> Dict[str, Any]:
        """Compute comprehensive identification statistics."""
        successful_trials = [t for t in all_trials if t.success]
        
        if not successful_trials:
            return {'error': 'No successful trials for statistics'}
        
        # Extract metrics
        id_rates = [t.identification_rate for t in successful_trials]
        fp_rates = [t.false_positive_rate for t in successful_trials]
        precisions = [t.precision for t in successful_trials]
        recalls = [t.recall for t in successful_trials]
        f1_scores = [t.f1_score for t in successful_trials]
        
        statistics = {
            'trial_summary': {
                'total_trials': len(all_trials),
                'successful_trials': len(successful_trials),
                'success_rate': len(successful_trials) / len(all_trials)
            },
            'identification_rate': {
                'mean': float(np.mean(id_rates)),
                'std': float(np.std(id_rates)),
                'median': float(np.median(id_rates)),
                'min': float(np.min(id_rates)),
                'max': float(np.max(id_rates)),
                'percentile_95': float(np.percentile(id_rates, 95))
            },
            'false_positive_rate': {
                'mean': float(np.mean(fp_rates)),
                'std': float(np.std(fp_rates)),
                'median': float(np.median(fp_rates)),
                'min': float(np.min(fp_rates)),
                'max': float(np.max(fp_rates))
            },
            'precision': {
                'mean': float(np.mean(precisions)),
                'std': float(np.std(precisions)),
                'median': float(np.median(precisions))
            },
            'recall': {
                'mean': float(np.mean(recalls)),
                'std': float(np.std(recalls)),
                'median': float(np.median(recalls))
            },
            'f1_score': {
                'mean': float(np.mean(f1_scores)),
                'std': float(np.std(f1_scores)),
                'median': float(np.median(f1_scores))
            }
        }
        
        # Performance breakdown by configuration
        unique_densities = sorted(set(t.n_catalog_stars for t in successful_trials))
        unique_mag_ranges = sorted(set(t.magnitude_range for t in successful_trials))
        unique_field_angles = sorted(set(t.field_angle_deg for t in successful_trials))
        
        if len(unique_densities) > 1:
            statistics['performance_vs_density'] = {}
            for density in unique_densities:
                density_trials = [t for t in successful_trials if t.n_catalog_stars == density]
                if density_trials:
                    density_id_rates = [t.identification_rate for t in density_trials]
                    statistics['performance_vs_density'][density] = {
                        'mean_id_rate': float(np.mean(density_id_rates)),
                        'std_id_rate': float(np.std(density_id_rates)),
                        'n_trials': len(density_trials)
                    }
        
        return statistics
    
    def _generate_identification_plots(
        self,
        all_trials: List[IdentificationTrial]
    ) -> Dict[str, Path]:
        """Generate identification performance plots."""
        plot_paths = {}
        
        # Identification rate distribution
        plot_paths['id_rate_distribution'] = self._plot_identification_distribution(all_trials)
        
        # Performance vs density
        plot_paths['performance_vs_density'] = self._plot_performance_vs_density(all_trials)
        
        # Confusion matrix heatmap
        plot_paths['confusion_heatmap'] = self._plot_confusion_heatmap(all_trials)
        
        return plot_paths
    
    def _plot_identification_distribution(self, trials: List[IdentificationTrial]) -> Path:
        """Plot identification rate distribution."""
        output_path = self.config.output_dir / "identification_rate_distribution.png"
        
        successful_trials = [t for t in trials if t.success]
        if not successful_trials:
            return output_path
        
        id_rates = [t.identification_rate for t in successful_trials]
        fp_rates = [t.false_positive_rate for t in successful_trials]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Identification rate histogram
        ax1.hist(id_rates, bins=30, alpha=0.7, edgecolor='black')
        ax1.axvline(np.mean(id_rates), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(id_rates):.3f}')
        ax1.set_xlabel('Identification Rate')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Identification Rate Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # False positive rate histogram
        ax2.hist(fp_rates, bins=30, alpha=0.7, edgecolor='black', color='orange')
        ax2.axvline(np.mean(fp_rates), color='red', linestyle='--',
                   label=f'Mean: {np.mean(fp_rates):.3f}')
        ax2.set_xlabel('False Positive Rate')
        ax2.set_ylabel('Frequency')
        ax2.set_title('False Positive Rate Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
    
    def _plot_performance_vs_density(self, trials: List[IdentificationTrial]) -> Path:
        """Plot performance vs star density."""
        output_path = self.config.output_dir / "performance_vs_density.png"
        
        successful_trials = [t for t in trials if t.success]
        if not successful_trials:
            return output_path
        
        # Group by density
        densities = sorted(set(t.n_catalog_stars for t in successful_trials))
        density_stats = {}
        
        for density in densities:
            density_trials = [t for t in successful_trials if t.n_catalog_stars == density]
            if density_trials:
                id_rates = [t.identification_rate for t in density_trials]
                fp_rates = [t.false_positive_rate for t in density_trials]
                density_stats[density] = {
                    'id_rate_mean': np.mean(id_rates),
                    'id_rate_std': np.std(id_rates),
                    'fp_rate_mean': np.mean(fp_rates),
                    'fp_rate_std': np.std(fp_rates)
                }
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Identification rate vs density
        densities_list = list(density_stats.keys())
        id_means = [density_stats[d]['id_rate_mean'] for d in densities_list]
        id_stds = [density_stats[d]['id_rate_std'] for d in densities_list]
        
        ax1.errorbar(densities_list, id_means, yerr=id_stds, marker='o', capsize=5)
        ax1.set_xlabel('Number of Stars in FOV')
        ax1.set_ylabel('Identification Rate')
        ax1.set_title('Identification Rate vs Star Density')
        ax1.grid(True, alpha=0.3)
        
        # False positive rate vs density
        fp_means = [density_stats[d]['fp_rate_mean'] for d in densities_list]
        fp_stds = [density_stats[d]['fp_rate_std'] for d in densities_list]
        
        ax2.errorbar(densities_list, fp_means, yerr=fp_stds, marker='s', capsize=5, color='orange')
        ax2.set_xlabel('Number of Stars in FOV')
        ax2.set_ylabel('False Positive Rate')
        ax2.set_title('False Positive Rate vs Star Density')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
    
    def _plot_confusion_heatmap(self, trials: List[IdentificationTrial]) -> Path:
        """Plot confusion matrix heatmap."""
        output_path = self.config.output_dir / "confusion_matrix_heatmap.png"
        
        successful_trials = [t for t in trials if t.success]
        if not successful_trials:
            return output_path
        
        # Aggregate confusion matrices
        total_tp = sum(t.n_matched_stars for t in successful_trials)
        total_fp = sum(t.n_false_positives for t in successful_trials)
        total_fn = sum(t.n_false_negatives for t in successful_trials)
        total_tn = 0  # Not well-defined for star identification
        
        confusion_matrix = np.array([[total_tp, total_fn], [total_fp, total_tn]])
        
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(confusion_matrix, interpolation='nearest', cmap='Blues')
        
        # Add text annotations
        for i in range(2):
            for j in range(2):
                text = ax.text(j, i, confusion_matrix[i, j], ha="center", va="center")
        
        ax.set_title('Aggregated Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        ax.set_xticks([0, 1])
        ax.set_yticks([0, 1])
        ax.set_xticklabels(['Identified', 'Not Identified'])
        ax.set_yticklabels(['Catalog Star', 'Background'])
        
        plt.colorbar(im)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
    
    def _plot_density_analysis(self, density_results: Dict[int, Dict[str, Any]]) -> Path:
        """Plot density analysis results."""
        output_path = self.config.output_dir / "density_analysis.png"
        
        densities = sorted(density_results.keys())
        id_rates = [density_results[d]['mean_identification_rate'] for d in densities]
        id_stds = [density_results[d]['std_identification_rate'] for d in densities]
        
        plt.figure(figsize=(10, 6))
        plt.errorbar(densities, id_rates, yerr=id_stds, marker='o', capsize=5, capthick=2)
        plt.xlabel('Number of Stars in FOV')
        plt.ylabel('Identification Rate')
        plt.title('Star Identification Rate vs Field Density')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1.1)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
    
    def _save_detailed_results(self, trials: List[IdentificationTrial]) -> Path:
        """Save detailed results to CSV."""
        output_path = self.config.output_dir / "identification_detailed_results.csv"
        
        # Convert trials to DataFrame
        trial_data = []
        for trial in trials:
            row = {
                'trial_id': trial.trial_id,
                'n_catalog_stars': trial.n_catalog_stars,
                'n_detected_stars': trial.n_detected_stars,
                'n_matched_stars': trial.n_matched_stars,
                'n_false_positives': trial.n_false_positives,
                'n_false_negatives': trial.n_false_negatives,
                'identification_rate': trial.identification_rate,
                'false_positive_rate': trial.false_positive_rate,
                'precision': trial.precision,
                'recall': trial.recall,
                'f1_score': trial.f1_score,
                'magnitude_min': trial.magnitude_range[0],
                'magnitude_max': trial.magnitude_range[1],
                'field_angle_deg': trial.field_angle_deg,
                'execution_time': trial.execution_time,
                'success': trial.success,
                'error_message': trial.error_message or ''
            }
            trial_data.append(row)
        
        df = pd.DataFrame(trial_data)
        df.to_csv(output_path, index=False)
        
        logger.info(f"Saved detailed results: {output_path}")
        return output_path
    
    def _analyze_false_positives(self, trials: List[IdentificationTrial]) -> Dict[str, Any]:
        """Analyze false positive patterns."""
        return self.false_positive_analysis(trials)

# Export main class
__all__ = ['IdentificationValidator', 'IdentificationConfig', 'IdentificationTrial']